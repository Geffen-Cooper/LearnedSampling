vrnn:
  hidden_size: 32

tinyhar:
  filter_num: 20

convlstm:
  nb_conv_blocks: 2
  nb_filters: 64
  dilation: 1
  batch_norm: 0
  filter_width: 3
  nb_layers_lstm: 1
  drop_prob: 0.5
  nb_units_lstm: 128
  filter_scaling_factor: 1

attend:
  hidden_dim: 128
  filter_num: 64
  filter_size: 3
  enc_is_bidirectional : False
  enc_num_layers: 2
  dropout: 0.5
  dropout_rnn: 0.25
  dropout_cls: 0.5
  activation: "ReLU"
  sa_div: 1